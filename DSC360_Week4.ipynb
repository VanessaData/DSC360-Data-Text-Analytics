{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Vanessa Williams\n",
        "DSC 360.Week 4"
      ],
      "metadata": {
        "id": "IvowdHc99Gln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Requests"
      ],
      "metadata": {
        "id": "Xoj9buhs1yzj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import request function because of the url needed for the assingment. This will send http requests with a clean, and simple interface"
      ],
      "metadata": {
        "id": "hWjsFBZO1-nX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wuTLsW11QLl",
        "outputId": "93bbb3fd-6bf9-4d63-cb3a-7dc3003952fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/bellevue-university/dsc360/refs/heads/main/12%20Week/week_4/big.txt'\n",
        "response = requests.get(url)\n",
        "\n",
        "# Save the file\n",
        "with open('big.txt', 'wb') as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "print(\"File downloaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing necessary libraries for text processing.\n"
      ],
      "metadata": {
        "id": "CiBTJEVm2vT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pandas is used for handling data, and nltk for stop words\n",
        "\n",
        "\n",
        "This class will have methods to clean text and remove stop words.\n",
        "\n",
        "Stop words are common words in the language that are usually removed during text preprocessing.\n",
        "\n",
        "The clean_text function takes in text and processes it to make it easier for analysis.\n",
        "\n",
        "Converting the text to lowercase so that it is case insensitive.\n",
        "\n",
        "Removing any numbers from the text as they are not useful for most NLP tasks.\n",
        "\n",
        "Removing punctuation marks from the text.\n",
        "\n",
        "Stripping leading/trailing whitespace and replacing multiple spaces with a single space.\n",
        "\n",
        "The remove_stopwords function removes common words like 'the', 'and', etc.\n",
        "\n",
        "Splitting the text into individual words (tokens).\n",
        "\n",
        "Filtering out any word that is in the stop_words list.\n",
        "\n",
        "Joining the remaining tokens back into a single string of text.\n",
        "\n",
        "The normalize_corpus function will clean and filter a Pandas Series, returning a single stream of cleaned text.\n",
        "\n",
        "Using apply and lambda to clean and filter the text in each row of the Pandas Series.\n",
        "\n",
        "Joining the entire cleaned series into one long string of text."
      ],
      "metadata": {
        "id": "i5l673RF248m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Defining the TextNormalizer class. This class will have methods to clean text and remove stop words.\n",
        "class TextNormalizer:\n",
        "\n",
        "    def __init__(self):\n",
        "        # Stop words are common words in the language that are usually removed during text preprocessing.\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # The clean_text function takes in text and processes it to make it easier for analysis.\n",
        "    def clean_text(self, text):\n",
        "        # Converting the text to lowercase so that it is case insensitive.\n",
        "        text = text.lower()\n",
        "        # Removing any numbers from the text as they are not useful for most NLP tasks.\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        # Removing punctuation marks from the text.\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        # Stripping leading/trailing whitespace and replacing multiple spaces with a single space.\n",
        "        text = text.strip()\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text\n",
        "\n",
        "    # The remove_stopwords function removes common words like 'the', 'and', etc.\n",
        "    def remove_stopwords(self, text):\n",
        "        # Splitting the text into individual words (tokens).\n",
        "        tokens = text.split()\n",
        "        # Filtering out any word that is in the stop_words list.\n",
        "        filtered_tokens = [token for token in tokens if token not in self.stop_words]\n",
        "        # Joining the remaining tokens back into a single string of text.\n",
        "        return ' '.join(filtered_tokens)\n",
        "\n",
        "    # The normalize_corpus function will clean and filter a Pandas Series, returning a single stream of cleaned text.\n",
        "    def normalize_corpus(self, series):\n",
        "        # Using apply and lambda to clean and filter the text in each row of the Pandas Series.\n",
        "        series = series.apply(self.clean_text)\n",
        "        series = series.apply(self.remove_stopwords)\n",
        "        # Joining the entire cleaned series into one long string of text.\n",
        "        return ' '.join(series)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhLF5oEu1foV",
        "outputId": "289e6ed6-dbdd-434e-ce51-3f04b09b5a96"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the big.txt file\n"
      ],
      "metadata": {
        "id": "yygKvETP5RxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the text into a Pandas Series\n",
        "\n",
        "Initialize the TextNormalizer class\n",
        "\n",
        "Normalize the text using the normalize_corpus method\n",
        "\n",
        "Print the first 1000 characters of the normalized text to verify\n",
        "\n",
        "Convert the text into a Pandas Series\n",
        "\n",
        "Initialize the TextNormalizer class\n",
        "\n",
        "Normalize the text using the normalize_corpus method\n",
        "\n",
        "Print the first 1000 characters of the normalized text to verify"
      ],
      "metadata": {
        "id": "5eq0Ctz45Z6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the big.txt file\n",
        "with open('big.txt', 'r') as file:\n",
        "    text_data = file.readlines()\n",
        "\n",
        "# Convert the text into a Pandas Series\n",
        "text_series = pd.Series(text_data)\n",
        "\n",
        "# Initialize the TextNormalizer class\n",
        "normalizer = TextNormalizer()\n",
        "\n",
        "# Normalize the text using the normalize_corpus method\n",
        "normalized_text = normalizer.normalize_corpus(text_series)\n",
        "\n",
        "# Print the first 1000 characters of the normalized text to verify\n",
        "print(normalized_text[:1000])\n",
        "with open('big.txt', 'r') as file:\n",
        "    text_data = file.readlines()\n",
        "\n",
        "# Convert the text into a Pandas Series\n",
        "text_series = pd.Series(text_data)\n",
        "\n",
        "# Initialize the TextNormalizer class\n",
        "normalizer = TextNormalizer()\n",
        "\n",
        "# Normalize the text using the normalize_corpus method\n",
        "normalized_text = normalizer.normalize_corpus(text_series)\n",
        "\n",
        "# Print the first 1000 characters of the normalized text to verify\n",
        "print(normalized_text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lx7V6UBy2qxY",
        "outputId": "3801b1bc-e8a4-4a36-9402-21e0570e15ff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "project gutenberg ebook adventures sherlock holmes sir arthur conan doyle series sir arthur conan doyle  copyright laws changing world sure check copyright laws country downloading redistributing project gutenberg ebook  header first thing seen viewing project gutenberg file please remove change edit header without written permission  please read legal small print information ebook project gutenberg bottom file included important information specific rights restrictions file may used also find make donation project gutenberg get involved   welcome world free plain vanilla electronic texts  ebooks readable humans computers since  ebooks prepared thousands volunteers   title adventures sherlock holmes  author sir arthur conan doyle  release date march ebook recently updated november  edition  language english  character set encoding ascii  start project gutenberg ebook adventures sherlock holmes     additional editing jose menendez    adventures sherlock holmes    sir arthur conan doyle \n",
            "project gutenberg ebook adventures sherlock holmes sir arthur conan doyle series sir arthur conan doyle  copyright laws changing world sure check copyright laws country downloading redistributing project gutenberg ebook  header first thing seen viewing project gutenberg file please remove change edit header without written permission  please read legal small print information ebook project gutenberg bottom file included important information specific rights restrictions file may used also find make donation project gutenberg get involved   welcome world free plain vanilla electronic texts  ebooks readable humans computers since  ebooks prepared thousands volunteers   title adventures sherlock holmes  author sir arthur conan doyle  release date march ebook recently updated november  edition  language english  character set encoding ascii  start project gutenberg ebook adventures sherlock holmes     additional editing jose menendez    adventures sherlock holmes    sir arthur conan doyle \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import spaCy"
      ],
      "metadata": {
        "id": "_bdHbe8C7-QD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the small English model for spaCy\n",
        "\n",
        "Process the first 1000 characters of the normalized text using spaCy's NLP pipeline\n",
        "\n",
        "Loop through each token in the text and print out the details"
      ],
      "metadata": {
        "id": "f-iX28QL8Dlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import spaCy\n",
        "import spacy\n",
        "\n",
        "# Load the small English model for spaCy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Process the first 1000 characters of the normalized text using spaCy's NLP pipeline\n",
        "doc = nlp(normalized_text[:1000])\n",
        "\n",
        "# Loop through each token in the text and print out the details\n",
        "for token in doc:\n",
        "    print(f\"Token: {token.text}, Lemma: {token.lemma_}, POS: {token.pos_}, Dependency: {token.dep_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03bKZjHu6Trc",
        "outputId": "13b8611e-215e-4d16-a05f-cdb4381b54fb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: project, Lemma: project, POS: PROPN, Dependency: compound\n",
            "Token: gutenberg, Lemma: gutenberg, POS: PROPN, Dependency: compound\n",
            "Token: ebook, Lemma: ebook, POS: NOUN, Dependency: nsubj\n",
            "Token: adventures, Lemma: adventure, POS: VERB, Dependency: nsubj\n",
            "Token: sherlock, Lemma: sherlock, POS: PROPN, Dependency: compound\n",
            "Token: holmes, Lemma: holmes, POS: PROPN, Dependency: compound\n",
            "Token: sir, Lemma: sir, POS: PROPN, Dependency: compound\n",
            "Token: arthur, Lemma: arthur, POS: PROPN, Dependency: compound\n",
            "Token: conan, Lemma: conan, POS: PROPN, Dependency: compound\n",
            "Token: doyle, Lemma: doyle, POS: PROPN, Dependency: compound\n",
            "Token: series, Lemma: series, POS: PROPN, Dependency: compound\n",
            "Token: sir, Lemma: sir, POS: PROPN, Dependency: compound\n",
            "Token: arthur, Lemma: arthur, POS: PROPN, Dependency: compound\n",
            "Token: conan, Lemma: conan, POS: PROPN, Dependency: compound\n",
            "Token: doyle, Lemma: doyle, POS: PROPN, Dependency: compound\n",
            "Token:  , Lemma:  , POS: SPACE, Dependency: dep\n",
            "Token: copyright, Lemma: copyright, POS: NOUN, Dependency: compound\n",
            "Token: laws, Lemma: law, POS: NOUN, Dependency: dobj\n",
            "Token: changing, Lemma: change, POS: VERB, Dependency: acl\n",
            "Token: world, Lemma: world, POS: NOUN, Dependency: dobj\n",
            "Token: sure, Lemma: sure, POS: ADV, Dependency: advmod\n",
            "Token: check, Lemma: check, POS: VERB, Dependency: ROOT\n",
            "Token: copyright, Lemma: copyright, POS: NOUN, Dependency: compound\n",
            "Token: laws, Lemma: law, POS: NOUN, Dependency: compound\n",
            "Token: country, Lemma: country, POS: NOUN, Dependency: nsubj\n",
            "Token: downloading, Lemma: download, POS: VERB, Dependency: acl\n",
            "Token: redistributing, Lemma: redistribute, POS: VERB, Dependency: amod\n",
            "Token: project, Lemma: project, POS: NOUN, Dependency: compound\n",
            "Token: gutenberg, Lemma: gutenberg, POS: PROPN, Dependency: dobj\n",
            "Token: ebook, Lemma: ebook, POS: VERB, Dependency: ccomp\n",
            "Token:  , Lemma:  , POS: SPACE, Dependency: dep\n",
            "Token: header, Lemma: header, POS: NOUN, Dependency: nmod\n",
            "Token: first, Lemma: first, POS: ADJ, Dependency: amod\n",
            "Token: thing, Lemma: thing, POS: NOUN, Dependency: dobj\n",
            "Token: seen, Lemma: see, POS: VERB, Dependency: conj\n",
            "Token: viewing, Lemma: view, POS: VERB, Dependency: amod\n",
            "Token: project, Lemma: project, POS: NOUN, Dependency: compound\n",
            "Token: gutenberg, Lemma: gutenberg, POS: PROPN, Dependency: compound\n",
            "Token: file, Lemma: file, POS: NOUN, Dependency: nsubj\n",
            "Token: please, Lemma: please, POS: INTJ, Dependency: intj\n",
            "Token: remove, Lemma: remove, POS: VERB, Dependency: ccomp\n",
            "Token: change, Lemma: change, POS: NOUN, Dependency: compound\n",
            "Token: edit, Lemma: edit, POS: NOUN, Dependency: compound\n",
            "Token: header, Lemma: header, POS: NOUN, Dependency: dobj\n",
            "Token: without, Lemma: without, POS: ADP, Dependency: prep\n",
            "Token: written, Lemma: write, POS: VERB, Dependency: amod\n",
            "Token: permission, Lemma: permission, POS: NOUN, Dependency: pobj\n",
            "Token:  , Lemma:  , POS: SPACE, Dependency: dep\n",
            "Token: please, Lemma: please, POS: INTJ, Dependency: intj\n",
            "Token: read, Lemma: read, POS: VERB, Dependency: conj\n",
            "Token: legal, Lemma: legal, POS: ADJ, Dependency: amod\n",
            "Token: small, Lemma: small, POS: ADJ, Dependency: amod\n",
            "Token: print, Lemma: print, POS: NOUN, Dependency: compound\n",
            "Token: information, Lemma: information, POS: NOUN, Dependency: compound\n",
            "Token: ebook, Lemma: ebook, POS: NOUN, Dependency: compound\n",
            "Token: project, Lemma: project, POS: NOUN, Dependency: compound\n",
            "Token: gutenberg, Lemma: gutenberg, POS: PROPN, Dependency: compound\n",
            "Token: bottom, Lemma: bottom, POS: PROPN, Dependency: compound\n",
            "Token: file, Lemma: file, POS: NOUN, Dependency: nsubj\n",
            "Token: included, Lemma: include, POS: VERB, Dependency: conj\n",
            "Token: important, Lemma: important, POS: ADJ, Dependency: amod\n",
            "Token: information, Lemma: information, POS: NOUN, Dependency: nmod\n",
            "Token: specific, Lemma: specific, POS: ADJ, Dependency: amod\n",
            "Token: rights, Lemma: right, POS: NOUN, Dependency: compound\n",
            "Token: restrictions, Lemma: restriction, POS: NOUN, Dependency: compound\n",
            "Token: file, Lemma: file, POS: NOUN, Dependency: dobj\n",
            "Token: may, Lemma: may, POS: AUX, Dependency: aux\n",
            "Token: used, Lemma: use, POS: VERB, Dependency: conj\n",
            "Token: also, Lemma: also, POS: ADV, Dependency: advmod\n",
            "Token: find, Lemma: find, POS: VERB, Dependency: xcomp\n",
            "Token: make, Lemma: make, POS: VERB, Dependency: xcomp\n",
            "Token: donation, Lemma: donation, POS: NOUN, Dependency: compound\n",
            "Token: project, Lemma: project, POS: NOUN, Dependency: compound\n",
            "Token: gutenberg, Lemma: gutenberg, POS: PROPN, Dependency: nsubj\n",
            "Token: get, Lemma: get, POS: AUX, Dependency: auxpass\n",
            "Token: involved, Lemma: involve, POS: VERB, Dependency: ccomp\n",
            "Token:   , Lemma:   , POS: SPACE, Dependency: dep\n",
            "Token: welcome, Lemma: welcome, POS: ADJ, Dependency: amod\n",
            "Token: world, Lemma: world, POS: NOUN, Dependency: npadvmod\n",
            "Token: free, Lemma: free, POS: ADJ, Dependency: amod\n",
            "Token: plain, Lemma: plain, POS: ADJ, Dependency: amod\n",
            "Token: vanilla, Lemma: vanilla, POS: NOUN, Dependency: nmod\n",
            "Token: electronic, Lemma: electronic, POS: ADJ, Dependency: amod\n",
            "Token: texts, Lemma: text, POS: NOUN, Dependency: compound\n",
            "Token:  , Lemma:  , POS: SPACE, Dependency: dep\n",
            "Token: ebooks, Lemma: ebook, POS: NOUN, Dependency: nmod\n",
            "Token: readable, Lemma: readable, POS: ADJ, Dependency: amod\n",
            "Token: humans, Lemma: human, POS: NOUN, Dependency: compound\n",
            "Token: computers, Lemma: computer, POS: NOUN, Dependency: dobj\n",
            "Token: since, Lemma: since, POS: SCONJ, Dependency: mark\n",
            "Token:  , Lemma:  , POS: SPACE, Dependency: dep\n",
            "Token: ebooks, Lemma: ebook, POS: NOUN, Dependency: nsubj\n",
            "Token: prepared, Lemma: prepare, POS: VERB, Dependency: advcl\n",
            "Token: thousands, Lemma: thousand, POS: NOUN, Dependency: nummod\n",
            "Token: volunteers, Lemma: volunteer, POS: NOUN, Dependency: compound\n",
            "Token:   , Lemma:   , POS: SPACE, Dependency: dep\n",
            "Token: title, Lemma: title, POS: NOUN, Dependency: compound\n",
            "Token: adventures, Lemma: adventure, POS: VERB, Dependency: ccomp\n",
            "Token: sherlock, Lemma: sherlock, POS: PROPN, Dependency: compound\n",
            "Token: holmes, Lemma: holmes, POS: PROPN, Dependency: compound\n",
            "Token:  , Lemma:  , POS: SPACE, Dependency: dep\n",
            "Token: author, Lemma: author, POS: NOUN, Dependency: compound\n",
            "Token: sir, Lemma: sir, POS: PROPN, Dependency: compound\n",
            "Token: arthur, Lemma: arthur, POS: PROPN, Dependency: compound\n",
            "Token: conan, Lemma: conan, POS: PROPN, Dependency: compound\n",
            "Token: doyle, Lemma: doyle, POS: NOUN, Dependency: compound\n",
            "Token:  , Lemma:  , POS: SPACE, Dependency: dep\n",
            "Token: release, Lemma: release, POS: NOUN, Dependency: compound\n",
            "Token: date, Lemma: date, POS: NOUN, Dependency: compound\n",
            "Token: march, Lemma: march, POS: PROPN, Dependency: compound\n",
            "Token: ebook, Lemma: ebook, POS: NOUN, Dependency: nsubj\n",
            "Token: recently, Lemma: recently, POS: ADV, Dependency: advmod\n",
            "Token: updated, Lemma: update, POS: VERB, Dependency: relcl\n",
            "Token: november, Lemma: november, POS: PROPN, Dependency: compound\n",
            "Token:  , Lemma:  , POS: SPACE, Dependency: dep\n",
            "Token: edition, Lemma: edition, POS: NOUN, Dependency: nmod\n",
            "Token:  , Lemma:  , POS: SPACE, Dependency: dep\n",
            "Token: language, Lemma: language, POS: NOUN, Dependency: nmod\n",
            "Token: english, Lemma: english, POS: PROPN, Dependency: amod\n",
            "Token:  , Lemma:  , POS: SPACE, Dependency: dep\n",
            "Token: character, Lemma: character, POS: NOUN, Dependency: compound\n",
            "Token: set, Lemma: set, POS: NOUN, Dependency: npadvmod\n",
            "Token: encoding, Lemma: encoding, POS: NOUN, Dependency: compound\n",
            "Token: ascii, Lemma: ascii, POS: NOUN, Dependency: dobj\n",
            "Token:  , Lemma:  , POS: SPACE, Dependency: dep\n",
            "Token: start, Lemma: start, POS: NOUN, Dependency: compound\n",
            "Token: project, Lemma: project, POS: PROPN, Dependency: compound\n",
            "Token: gutenberg, Lemma: gutenberg, POS: PROPN, Dependency: compound\n",
            "Token: ebook, Lemma: ebook, POS: NOUN, Dependency: nsubj\n",
            "Token: adventures, Lemma: adventure, POS: VERB, Dependency: ccomp\n",
            "Token: sherlock, Lemma: sherlock, POS: PROPN, Dependency: compound\n",
            "Token: holmes, Lemma: holmes, POS: PROPN, Dependency: dobj\n",
            "Token:     , Lemma:     , POS: SPACE, Dependency: dep\n",
            "Token: additional, Lemma: additional, POS: ADJ, Dependency: amod\n",
            "Token: editing, Lemma: editing, POS: NOUN, Dependency: compound\n",
            "Token: jose, Lemma: jose, POS: ADJ, Dependency: amod\n",
            "Token: menendez, Lemma: menendez, POS: PROPN, Dependency: compound\n",
            "Token:    , Lemma:    , POS: SPACE, Dependency: dep\n",
            "Token: adventures, Lemma: adventure, POS: VERB, Dependency: ccomp\n",
            "Token: sherlock, Lemma: sherlock, POS: PROPN, Dependency: compound\n",
            "Token: holmes, Lemma: holmes, POS: PROPN, Dependency: dobj\n",
            "Token:    , Lemma:    , POS: SPACE, Dependency: dep\n",
            "Token: sir, Lemma: sir, POS: PROPN, Dependency: compound\n",
            "Token: arthur, Lemma: arthur, POS: PROPN, Dependency: compound\n",
            "Token: conan, Lemma: conan, POS: PROPN, Dependency: compound\n",
            "Token: doyle, Lemma: doyle, POS: PROPN, Dependency: dobj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary of the Process:"
      ],
      "metadata": {
        "id": "8mTKkDD38cRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment, I used spaCy for tokenization, lemmatization, part-of-speech (POS) tagging, and syntactic dependency parsing. While I initially intended to compare the output of spaCy with NLTK, I encountered persistent issues with downloading and configuring NLTK resources, specifically the averaged_perceptron_tagger. As a result, I proceeded with spaCy, which handled all the required tasks efficiently."
      ],
      "metadata": {
        "id": "Z5w8ODkr8fWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results with spaCy:"
      ],
      "metadata": {
        "id": "m2IJ7Bvp8mZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using spaCy, I processed the first 1,000 characters of the normalized text. The output included tokens, their lemmatized forms, part-of-speech tags, and syntactic dependencies. For example:\n",
        "\n",
        "-Token: project, Lemma: project, POS: PROPN, Dependency: compound\n",
        "\n",
        "-Token: gutenberg, Lemma: gutenberg, POS: PROPN, Dependency: compound\n",
        "\n",
        "-Token: ebook, Lemma: ebook, POS: NOUN, Dependency: nsubj"
      ],
      "metadata": {
        "id": "RLnVGvCK8rCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ..."
      ],
      "metadata": {
        "id": "vEJSBvKJ87oP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although I could not compare these results with NLTK, spaCy proved to be an effective and reliable tool for text analysis in this context."
      ],
      "metadata": {
        "id": "5_TU1ZFI9ArQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CfAac8b789ht"
      }
    }
  ]
}